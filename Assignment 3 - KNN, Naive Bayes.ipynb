{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 4580/5580 - Data Science – Fall 2020\n",
    "## Assignment 3: kNN, Naive Bayes and Text Feature Selection\n",
    "\n",
    "#### Student Full Name: Sharvita Paithankar\n",
    "#### Student ID: 108172438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables\n",
    "Complete all the exercises below and turn in a write-up in the form of a Jupyuter notebook, that is, an **.ipynb file**.\n",
    "The write-up should include your code, answers to exercise questions, and plots of results.\n",
    "You will submit your assignment online as an attachment, through Canvas under Assignment 3. \n",
    "\n",
    "You have to use this notebook and fill in answers inline.\n",
    "Don't forget to include answers to questions that ask for natural language responses, i.e., in English, not code!\n",
    "\n",
    "Please do not add or remove any cells (you can add cells while doing the homework but remove them before submission)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need Help? \n",
    "In case you need help with this assignment, please ask your questions on the designated channel for Assignment 3 on Microsoft Teams. This way, you can receive assistance from the TA’s or your classmates (<b>DO NOT share your solution</b>) [Recommended]. \n",
    "<br/>If you need to include your solution in your question, please create a new chat with <b>all the TA’s</b> (in the new chat, add all the TA’s rather than sending your question to each TA separately)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration\n",
    "This assignment is to be done individually. Everyone should obtain hands-on experience in this course. You are free to discuss course material with fellow students, and we encourage you to use the resources in the Internet for your understanding, but the work you turn in, including all code and answers, must be your own work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Get Started!\n",
    "===\n",
    "## Overview\n",
    "\n",
    "Supervised Learning is the process of learning based on known examples which have both an input and a labeled output. Using the data and labels provided in the training dataset, models can learn a prediction function or mapping, which may then be applied to unseen test data. As we discussed in the class, some common methods for supervised learning include the K-Nearest-Neighbors Classifier (KNN) and Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning on Text Data\n",
    "\n",
    "For this assignment, we'll explore kNN and Naive Bayes further and look at different feature selection strategies. First of all, download the dataset for this assignment from Canvas and unpack into an \"Assignment 3\" directory. \n",
    "\n",
    "The dataset is from the RCV1 v2 corpus which is a collection of news articles and category labels. There are 103 categories, and each document can lie in one or more categories. \n",
    "\n",
    "The sparse train and test matrices are encoded as nnz x 3 integer matrices, where nnz is the total number of words in all docs. Each row of these matrices contains (doc, word, count) triples. We will store those in sparse matrices whose dimensions are ndocs x nwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "\n",
    "Your assignment is to perform supervised learning using both the KNN and Naive Bayes classifiers, and evaluate the results. You will also perform multiple types of text feature selection to choose subsets of the total features to use for classification, and evaluate these results as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read the data files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwords = np.loadtxt(\"data/words.imat.txt\",dtype=np.int32,)          # training data matrix in nnz x 3 form - rows are (doc, word, count) triples\n",
    "cats = np.loadtxt(\"data/cats.imat.txt\",dtype=np.int32)             # training labels in an ndocs x ncats matrix\n",
    "tiwords = np.loadtxt(\"data/testwords.imat.txt\",dtype=np.int32)     # test data matrix in nnz x 3 form\n",
    "tcats = np.loadtxt(\"data/testcats.imat.txt\",dtype=np.int32)        # testing labels in an ndocs x ncats matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iwords and tiwords encode the sparse train and test matrices. Each row of these matrices contains (doc, word, count) triples. We will store those in sparse matrices whose dimensions are ndocs x nwords. The exact matrix type is CSR which stands for compressed sparse rows. Look up its description here: https://en.wikipedia.org/wiki/Sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwords = iwords[:,2]\n",
    "train = sp.csr_matrix((fwords, (iwords[:,1], iwords[:,0])))\n",
    "ndocs = train.shape[0]\n",
    "nwords = train.shape[1]\n",
    "\n",
    "test = sp.csr_matrix((tiwords[:,2], (tiwords[:,1], tiwords[:,0])),shape=(4000,nwords))  # need to match the number of cols (words)\n",
    "ntdocs = test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the dataset has 103 category labels, we'll focus on training a single category. We will use Category 6, which is fairly evenly balanced between positives and negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat6 = cats[:,6]\n",
    "tcat6 = tcats[:,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: KNN Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we'll train Scikit-Learn's builtin kNN classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnc = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then \"fit\" it to the data. Notice how fast this is! Of course, training kNN is basically a no-op, and predicting is where all the work lies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnc.fit(train, cat6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try predicting. This will actually take some time, but hopefully only a few tens of seconds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = knnc.predict(test);preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the accuracy on the actual test set labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82225"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tcat6 == preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but kNN is very sensitive to the distance function as we discussed in class. Let's try first normalizing each row of \"train\" and \"test\" by the L2-norm of that row, i.e. we will divide by the square root of the sum of the squared word counts in the row. Comparing the normalized documents should do much better. \n",
    "\n",
    "Remember, the L2-Norm for a vector $x$ can be written as:\n",
    "\n",
    "$$  \\lVert x \\rVert_2  = \\sqrt{x_1^2 + x_1^2 + \\cdots + x_n^2}$$\n",
    "\n",
    "## Q1 [20 points]: \n",
    "\n",
    "Create two new variables `trainnmat` and `testnmat` which contain the L2-normalized versions of `train` and `test` respectively.\n",
    "\n",
    "L2-normalization should be performed across each row $r$ of the `train` and `test` matrices, so that:\n",
    "\n",
    "$$ \\sqrt{r_1^2 + r_2^2 + \\cdots + r_n^2} = 1 $$\n",
    "\n",
    "However, you must compute the L2-normalized matrices using `numpy` and `scipy.sparse` operations, using the scikit-learn `normalize` function to generate your answer is not valid.\n",
    "Also ensure that your output matrix is still sparse. Many NumPy operations will either fail or convert CSR matrices to dense matrices. We prefer to keep this matrix in CSR format for the duration of this lab.\n",
    "\n",
    "The documentation for the following sparse matrix function may be useful for you:\n",
    "\n",
    "- `scipy.sparse.csr_matrix` [Docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)\n",
    "- `scipy.sparse.csr_matrix.sum` [Docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.sum.html)\n",
    "- `scipy.sparse.csr_matrix.power` [Docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.power.html#scipy.sparse.csr_matrix.power)\n",
    "- `scipy.sparse.spdiags` [Docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.spdiags.html)\n",
    "\n",
    "Above we have already imported `scipy.sparse` as `sp`.\n",
    "\n",
    "**HINT:** For both `train` and `test` you need to structure your normalization as a matrix multiply operation. Start by creating a diagonal matrix where the squared sums of each row are along the diagonal elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 31331)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainnmat = <your code here>\n",
    "#testnmat = <your code here>\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from numpy.linalg import inv\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "norm_train= train.multiply(train).sum(1)\n",
    "testnorm = test.multiply(test).sum(1)\n",
    "trainf = 1.0 / np.sqrt(np.max(norm_train,1))\n",
    "testf = 1.0 / np.sqrt(np.max(testnorm,1));trainf.shape\n",
    "trainnmat = sp.spdiags(np.transpose(trainf), 0, ndocs, ndocs) * train\n",
    "testnmat = sp.spdiags(np.transpose(testf), 0, ntdocs, ntdocs) * test\n",
    "testnmat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CASES - DO NOT MODIFY\n",
    "# These test cases check that your normalization is close (within numerical precision) of the answer\n",
    "# from sklearn's normalize function.\n",
    "from sklearn.preprocessing import normalize\n",
    "skl_norm_train = normalize(train)\n",
    "skl_norm_train.sort_indices()\n",
    "\n",
    "skl_norm_test = normalize(test)\n",
    "skl_norm_test.sort_indices()\n",
    "\n",
    "trainnmat.sort_indices()\n",
    "testnmat.sort_indices()\n",
    "\n",
    "assert(np.allclose(trainnmat.data,skl_norm_train.data))\n",
    "assert(np.allclose(testnmat.data,skl_norm_test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train and evaluate a knn model using the normalized datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90275"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnc.fit(trainnmat, cat6)\n",
    "preds = knnc.predict(testnmat);preds\n",
    "np.mean(tcat6 == preds)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 [10 points]: \n",
    "What is the accuracy of your kNN classifier on the normalized data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the accuracy of kNN classifier on the normalized data is 0.90275"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Multinomial Naive Bayes classifier. The multinomial distribution models each word in a document as being drawn from a category-specific probability distribution over the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the NB model, which simply means tabulating the word counts for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(train, cat6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and predict the category labels for cat6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = mnb.predict(test);preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 [10 points]: \n",
    "\n",
    "Let's check the accuracy below. How did NB do compared to kNN? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8975"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tcat6 == preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Answer this question here\n",
    "#The accuracy is better with kNN comapres to the NB beacuse kNN had the accuracy of 0.90275"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Frequency-Based Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We argued that frequency-based feature selection is useful approach to culling features to reduce the size of a model. In some cases, accuracy improves as well. \n",
    "\n",
    "Let's construct a matrix rcounts which contains the counts of features from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3728, 4438, 3488, ...,    1,    1,    1]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcounts = (train>0).sum(0)\n",
    "rcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31331"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcounts.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll find the indices of non-zero elements of the predicate (rcounts >= count). These are the features that occurred at least count times. ii is the indices of these features.\n",
    "\n",
    "We then extract the columns of the train and test matrices corresponding to those high-count features. The last line tells us how many features we kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18330"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 2\n",
    "ii0 = np.asarray(np.nonzero(rcounts >= threshold)[1])\n",
    "ii = ii0.reshape(ii0.size,)\n",
    "\n",
    "strain = train[:,ii]\n",
    "stest = test[:,ii]\n",
    "ii.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we run training and evaluation on train/test data restricted to those columns to get an accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8955"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(strain, cat6)\n",
    "preds = mnb.predict(stest);preds\n",
    "np.mean(tcat6 == preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 [20 points]\n",
    "For the following question, use the `threshold` variable set above.\n",
    "\n",
    "For threshold counts of 1, 2, 5, 10, 20, tabulate the count, number of features kept, and the accuracy score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Features</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>18330</td>\n",
       "      <td>0.89550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>18330</td>\n",
       "      <td>0.89550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>9632</td>\n",
       "      <td>0.89000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>6169</td>\n",
       "      <td>0.88675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>4046</td>\n",
       "      <td>0.88400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Count  Features    Score\n",
       "0      1     18330  0.89550\n",
       "1      2     18330  0.89550\n",
       "2      5      9632  0.89000\n",
       "3     10      6169  0.88675\n",
       "4     20      4046  0.88400"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Answer the question here\n",
    "import pandas as pd\n",
    "\n",
    "mnb.fit(strain, cat6)\n",
    "preds = mnb.predict(stest);preds\n",
    "np.mean(tcat6 == preds)\n",
    "\n",
    "thresholds = [2,5,10,20]\n",
    "li = [[1, ii.size, np.mean(tcat6 == preds)]]\n",
    "for t in thresholds:\n",
    "    threshold = t\n",
    "    ii0 = np.asarray(np.nonzero(rcounts >= threshold)[1])\n",
    "    ii = ii0.reshape(ii0.size,)\n",
    "    strain = train[:,ii]\n",
    "    stest = test[:,ii]\n",
    "    ii.size\n",
    "    mnb.fit(strain, cat6)\n",
    "    preds = mnb.predict(stest);preds\n",
    "    np.mean(tcat6 == preds)\n",
    "    li.append([t, ii.size, np.mean(tcat6 == preds)])\n",
    "\n",
    "\n",
    "df3 = pd.DataFrame(li,columns=['Count','Features', 'Score'])\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 [10 points]\n",
    "\n",
    "How did frequency-based feature selection affect accuracy? How much was the model size (proportional to words kept) reduced? \n",
    "\n",
    "(`ii.size` is the reduced model size, `rcounts.size` is the original model size.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Answer the question here\n",
    "#The more features yielded higher accuracy than lower number of features. Model size was reduced by almost half each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Feature Selection Based on Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a more sophisticated method to select the (word) features. Since we have count data, the Chi-Squared test is appropriate. Remember that Chi-squared can be used on 2x2 contingency tables to determine if there is an \"interaction\". In this case, we test if the label interacts with the feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcounts = np.zeros((4,nwords))           # holds the counts of various combinations of label and feature\n",
    "expected = np.zeros((4,nwords))          # holds the predicted counts of those same combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcat6 = cat6.reshape(1,ndocs)\n",
    "fcounts[0,:] = mcat6 * (train > 0)                                   # label and word both = 1\n",
    "fcounts[1,:] = (1-mcat6) * (train > 0)                               # label = 0, word = 1\n",
    "fcounts[2,:] = np.sum(mcat6) * np.ones((1,nwords)) - fcounts[0,:]    # label = 1, word = 0\n",
    "fcounts[3,:] = np.sum(1-mcat6) * np.ones((1,nwords)) - fcounts[1,:]  # label = 0, word = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the definition of the CHI-squared test here:\n",
    "https://en.wikipedia.org/wiki/Chi-squared_test ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "allf = np.sum(fcounts,axis=0)\n",
    "pword0 = (fcounts[2,:] + fcounts[3,:]) / allf    # prob word = 0\n",
    "pword1 = (fcounts[0,:] + fcounts[1,:]) / allf    # prob word = 1\n",
    "pcat0 = (fcounts[1,:] + fcounts[3,:]) / allf     # prob label = 0\n",
    "pcat1 = (fcounts[0,:] + fcounts[2,:]) / allf     # prob label = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If label and word occurence are independent, then the expected count should be the product of the total count and the two corresponding probabilities:\n",
    "\n",
    "\n",
    "\n",
    "## Q6 [20 points]: \n",
    "\n",
    "Implement the expected counts here. \n",
    "\n",
    "**HINT** This can be computed entirely with the variables from the previous code cell, with no other variables needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implement this\n",
    "expected[0,:] = allf * pcat1 * pword1  \n",
    "expected[1,:] = allf * pcat0 * pword1 \n",
    "expected[2,:] = allf * pcat1 * pword0  \n",
    "expected[3,:] = allf * pcat0 * pword0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the chi-squared statistic value, we want the sum of the squared differences between actual and predicted counts, normalized by the expected counts. The following two cells perform that calculation. We have to guard against divide-by-zero so we use a small lower-bound on the denominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = fcounts - expected\n",
    "safeexpected = np.maximum(expected,1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31331,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chisquaredstat = np.sum(np.multiply(diff, diff) / safeexpected, axis=0)\n",
    "chisquaredstat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 83.64630905,  97.8608861 , 660.13078932, ...,   0.8417048 ,\n",
       "         0.8417048 ,   0.8417048 ])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chisquaredstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the chi-squared statistic to choose the best features. The CHI-squared stastistic is a measure of interaction between a word feature and the label. The higher its value, the better that word should be as a predictor of the category. We set a threshold and take the words whose chi-squared statistic is above this value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31281,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chisqthreshold = 0.001\n",
    "ii0 = np.asarray(np.nonzero(chisquaredstat > chisqthreshold)[0])\n",
    "ii = ii0.reshape(ii0.size,);ii.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we use the indices of good features selected by the chi-squared filter to train and evaluate the Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.898"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strain = train[:,ii]\n",
    "stest = test[:,ii]\n",
    "mnb.fit(strain, cat6)\n",
    "preds = mnb.predict(stest);preds\n",
    "np.mean(tcat6 == preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 [10 points]\n",
    "\n",
    "Using a chi-sqaured critical value table (one can be found here: [Link](https://people.richland.edu/james/lecture/m170/tbl-chi.html)), find the table row for 1 degree of freedom (`df=1`), and set `chisqthreshold` to a few different values from the table. Record your accuracies here.\n",
    "\n",
    "In a few sentences describe how well this feature selection process works on the test data.\n",
    "\n",
    "\n",
    "Answer :\n",
    "chisqthreshold -> accuracy \n",
    "0.005 -> 0.89775\n",
    "0.01 -> 0.8975\n",
    "0.025 -> 0.89725\n",
    "0.10 -> 0.8937\n",
    "0.90 -> 0.8945\n",
    "0.95->0.895\n",
    "0.975 -> 0.8947\n",
    "0.995 -> 0.89475\n",
    "\n",
    "The feature selection works well with accuracy near 0.8 -0.9. But I noticed that when the threshold increases, the accuracy decreases. This maybe beacuse some of the chi-square stats are above that certain threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit your Assignment 3 notebook on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
